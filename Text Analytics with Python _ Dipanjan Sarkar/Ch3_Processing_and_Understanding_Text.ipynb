{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually text corpora and other textual data in their native raw format are not well formatted and standardized, and of course, we should expect this—after all, text data is highly unstructured! \n",
    "- Text processing, or to be more specific, pre-processing, involves using a variety of techniques to convert raw text into well- defined sequences of linguistic components that have standard structure and notation.\n",
    "- The following list gives us an idea of some of the most popular text pre-processing techniques that we will be exploring in this chapter:\n",
    "    - Tokenization\n",
    "    - Tagging\n",
    "    - Chunking\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "- An important thing to remember always is that a robust text pre-processing system is always an essential part of any application on NLP and text analytics. \n",
    "    - The primary reason for that is because all the textual components that are obtained after pre-processing—be they words, phrases, sentences, or any other tokens—form the basic building blocks of input that are fed into the further stages of the application that perform more complex analyses, including learning patterns and extracting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Tokenization\n",
    "- __Tokens__ : independent and minimal textual components that have some definite syntax and semantics.\n",
    "- tokenization can be defined as the process of breaking down or splitting textual data into smaller meaningful components called tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Sentence Tokenization\n",
    "- The process of splitting a text corpus into sentences that act as the first level of tokens which the corpus is comprised of. \n",
    "- This is also known as sentence segmentation, because we try to segment the text into meaningful sentences. \n",
    "- Any text corpus is a body of text where each paragraph comprises several sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 144395)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "type(alice), len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = 'We will discuss briefly about the basic syntax, structure and \\\n",
    "design philosophies. There is a defined hierarchical syntax for Python code \\\n",
    "which you should remember when writing code! Python is a really powerful \\\n",
    "programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_st = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
       " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\",\n",
       " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.',\n",
       " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
       " 'Oh dear!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sentences = default_st(alice)\n",
    "alice_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
       " 'There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
       " 'Python is a really powerful programming language!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences = default_st(sample_text)\n",
    "sample_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1625)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total sentences\n",
    "len(sample_sentences), len(alice_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __The tokenizer considers other punctuation and the capitalization of words.__\n",
    "- We can also tokenize text of other languages. \n",
    "    - If we are dealing with German text, we can use _sent-tokenize_, which is already trained, or load a pre-trained tokenization model on German text into a _PunktSentenceTokenizer_ instance and perform the same operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157171,\n",
       " ' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other Language(German) example\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "len(german_text), german_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "german_sentences_def[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tokenize.punkt.PunktSentenceTokenizer"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "# verify the type of german_tokenizer\n",
    "# should be PunktSentenceTokenizer\n",
    "type(german_tokenizer) \n",
    "# -> PunktSentenceTokenizer, which is specialized in dealing with the German language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "german_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences_def == german_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sentences = punkt_st.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
       " 'There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
       " 'Python is a really powerful programming language!']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RegexpTokenizer class to tokenize text into sentences where we will use specific regular expression-based patterns to segment sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
       " 'There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
       " 'Python is a really powerful programming language!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "sample_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word Tokenization\n",
    "- Word tokenization is the process of splitting or segmenting sentences into their constituent words. \n",
    "- A sentence is a collection of words, and with tokenization we essentially split a sentence into a list of words that can be used to reconstruct the sentence. \n",
    "- very important in many processes, especially in cleaning and normalizing text where operations like stemming and lemmatization work on each individual word based on its respective stems and lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) word_tokenize\n",
    "- the default and recommended word tokenizer as specified by nltk. \n",
    "- This tokenizer is actually an instance or object of the __TreebankWordTokenizer__ class in its internal implementation and acts as a wrapper to that core class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) TreebankWordTokenizer\n",
    "- based on the Penn Treebank and uses various regular expressions to tokenize the text. \n",
    "- Of course, one primary assumption here is that we have already performed sentence tokenization beforehand. \n",
    "- Some of the main features of this tokenizer include the following:\n",
    "    - Splits and separates out periods that appear at the end of a sentence\n",
    "    - Splits and separates commas and single quotes when followed by whitespaces\n",
    "    - Most punctuation characters are split and separated into independent tokens\n",
    "    - Splits words with standard contractions—examples would be don’t to do and n’t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "words # output is similar to word_tokenize() because both use the same tokenizing mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if set to True, is used to find the gaps between the tokens. Otherwise, it is used to find the tokens themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'wasn',\n",
       " 't',\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify tokens themselves\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " \"wasn't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " \"couldn't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify gaps in tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get start and end indices of each token\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3),\n",
       " (4, 9),\n",
       " (10, 13),\n",
       " (14, 20),\n",
       " (21, 25),\n",
       " (26, 31),\n",
       " (32, 35),\n",
       " (36, 38),\n",
       " (39, 47),\n",
       " (48, 51),\n",
       " (52, 55),\n",
       " (56, 60)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " \"wasn't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " \"couldn't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence[start:end] for start, end in word_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iiii) WordPunktTokenizer\n",
    "- uses the pattern __r'\\w+|[^\\w\\s]+'__ to tokenize sentences into independent alphabetic and non-alphabetic tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer() # pattern = r'\\w+|[^\\w\\s]+'\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iiiii) WhitespaceTokenizer\n",
    "- whitespace : tabs, newlines, and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " \"wasn't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " \"couldn't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)  Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Case Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Correcting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding Text Syntax and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
