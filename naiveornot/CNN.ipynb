{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join('review_data','ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "with open(file_path, 'r') as f:\n",
    "    next(f) # header skip\n",
    "    for line in f.readlines():\n",
    "        _, doc, label = line.strip().split('\\t')\n",
    "        sentences.append(doc.strip())\n",
    "        labels.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) # 전체 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어릴때보고 지금다시봐도 재밌어요ㅋㅋ',\n",
       " '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.',\n",
       " '폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.',\n",
       " '와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지',\n",
       " '안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twi_tagger = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 품사 중 명사, 동사, 형용사, 부사, 감탄사(헐, 어머나), 한국어약어(ㅋㅋㅋ) 만 추출해봄.\n",
    "Non_Stop_words = set([\"Noun\", \"Verb\",\"Adjective\",\"Adverb\",\"Exclamation\",\"KoreanParticle\"])\n",
    "\n",
    "# (돈, Noun) -> \"돈/Noun\" 형태로 형태소 분석.\n",
    "def tokenizer_twit(doc, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        word_list = ['/'.join(t) for t in twi_tagger.pos(doc, norm=True, stem=True) if t[-1] in Non_Stop_words]\n",
    "    else:\n",
    "        word_list = ['/'.join(t) for t in twi_tagger.pos(doc, norm=True, stem=True)]\n",
    "    return word_list\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['디자인을',\n",
       " '배우는',\n",
       " '학생으로',\n",
       " ',',\n",
       " '외국디자이너와',\n",
       " '그들이',\n",
       " '일군',\n",
       " '전통을',\n",
       " '통해',\n",
       " '발전해가는',\n",
       " '문화산업이',\n",
       " '부러웠는데',\n",
       " '.',\n",
       " '사실',\n",
       " '우리나라에서도',\n",
       " '그',\n",
       " '어려운시절에',\n",
       " '끝까지',\n",
       " '열정을',\n",
       " '지킨',\n",
       " '노라노',\n",
       " '같은',\n",
       " '전통이있어',\n",
       " '저와',\n",
       " '같은',\n",
       " '사람들이',\n",
       " '꿈을',\n",
       " '꾸고',\n",
       " '이뤄나갈',\n",
       " '수',\n",
       " '있다는',\n",
       " '것에',\n",
       " '감사합니다',\n",
       " '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['디자인/Noun',\n",
       " '배우다/Verb',\n",
       " '학생/Noun',\n",
       " '외국/Noun',\n",
       " '디자이너/Noun',\n",
       " '그/Noun',\n",
       " '일군/Noun',\n",
       " '전통/Noun',\n",
       " '통해/Noun',\n",
       " '발전/Noun',\n",
       " '하다/Verb',\n",
       " '문화/Noun',\n",
       " '산업/Noun',\n",
       " '부럽다/Adjective',\n",
       " '사실/Noun',\n",
       " '우리나라/Noun',\n",
       " '그/Noun',\n",
       " '어렵다/Adjective',\n",
       " '시절/Noun',\n",
       " '끝/Noun',\n",
       " '열정/Noun',\n",
       " '지키다/Verb',\n",
       " '노라노/Noun',\n",
       " '같다/Adjective',\n",
       " '전통/Noun',\n",
       " '있다/Adjective',\n",
       " '저/Noun',\n",
       " '같다/Adjective',\n",
       " '사람/Noun',\n",
       " '꿈/Noun',\n",
       " '꾸다/Verb',\n",
       " '이루다/Verb',\n",
       " '나가다/Verb',\n",
       " '수/Noun',\n",
       " '있다/Adjective',\n",
       " '것/Noun',\n",
       " '감사/Noun',\n",
       " '하다/Verb']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_twit(sentences[1], remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement_num : 10000\n",
      "statement_num : 20000\n"
     ]
    }
   ],
   "source": [
    "words_list=[]\n",
    "statement_num=0\n",
    "for sent in sentences:\n",
    "    words_list.append(tokenizer_twit(sent, remove_stopwords=False))\n",
    "#     words_list.append(tokenizer(sent))\n",
    "    statement_num += 1\n",
    "    if statement_num%10000 == 0:\n",
    "        print(\"statement_num : %d\" % statement_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, save, output_file\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh import palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = multiprocessing.cpu_count() # Number of threads to run in parallel\n",
    "context = 3 # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "negative = 20\n",
    "iter_ = 20 # 얼마나 반복할지.\n",
    "sg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time() # 작업시간 검사용!\n",
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(words_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, negative=negative, iter=iter_, sg=sg)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(model.wv.index2word) # 전체 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_name = os.path.join('w2v_file','300features_10minwords_3context_twi2')\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 모델 로드.\n",
    "# model_name = os.path.join('w2v_file','300features_10minwords_3context_twi')\n",
    "# model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vis_top_n(model, filepath, top, lr=500, n_iter=1000, perplexity=10):\n",
    "    \"\"\"\n",
    "    전체 단어 중 특정 빈도 이상의 단어만 시각화.\n",
    "    :param model: gensim word2vec model\n",
    "    :param filepath: 시각화파일(html) 저장할 파일 경로\n",
    "    :param top: 빈도 상위 몇개의 단어까지 시각화할지\n",
    "    \"\"\"\n",
    "    vectors = model.wv.syn0[:top]\n",
    "    labels = model.wv.index2word[:top]\n",
    "    counts = [model.wv.vocab[label].count for label in labels]  # _labels의 빈도수 (빈도수별 사이즈 다르게 하기 위해)\n",
    "\n",
    "    if np.shape(vectors)[1] > 2:  # 입력값이 2차원일경우에는 차원축소 하지 않음.\n",
    "        print('tsne....')\n",
    "        start = time.time()\n",
    "        tsne = TSNE(perplexity=perplexity, n_components=2, init='random', n_iter=n_iter, verbose=1, learning_rate=lr, method='exact')  # tsne를 이용한 차원 축소 (n차원 -> 2차원)\n",
    "        vectors = tsne.fit_transform(vectors)\n",
    "        print(time.time()-start)\n",
    "\n",
    "    _filepath = '%s[top_%i].html' % (filepath, top)\n",
    "\n",
    "    source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=vectors.T[0],\n",
    "            y=vectors.T[1],\n",
    "            size=(np.log1p(np.array(counts))) * 1.2 + 2,\n",
    "            word=labels,\n",
    "            color=['#0099ff'] * len(labels),\n",
    "        )\n",
    "    )\n",
    "    label_set = LabelSet(x='x', y='y', text='word', level='glyph', text_color=\"#111111\", text_alpha=0.6, text_font_size='8pt', x_offset=5, y_offset=5, source=source, render_mode='canvas')\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,resize\"\n",
    "    p = figure(plot_width=900, plot_height=900, tools=[tools], title='word2vec vis top %i' % top)\n",
    "    p.circle('x', 'y', size='size', source=source, alpha=0.6, fill_color='color', line_color='#eeeeee')\n",
    "    p.add_layout(label_set)\n",
    "\n",
    "    output_file(_filepath, title=_filepath)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vis_top_n(model=model, filepath=os.path.join('w2v_file','300features_10minwords_3context_twi'), top=2000, lr=500, n_iter=1000, perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import losses\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_dics = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequences = [[model.wv.index2word.index(word) for word in wl if word in model.wv.index2word] for wl in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) # MAX_SEQUENCE_LENGTH만큼 뒤 기준으로 잘림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels)) # label one-hot encoding\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "indices[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(indices) # 데이터 셔플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[indices] # 데이터 셔플링\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(model.wv.index2word) + 1, EMBEDDING_DIM)) # bias term 때문에 +1 해준듯.. \n",
    "embedding_matrix # 랜덤값으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : 랜덤값 -1~1로 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, word in enumerate(model.wv.index2word):\n",
    "    embedding_matrix[i] = model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) A simplified Convolutional\n",
    "- Simply use total 128 filters with size 5 and max pooling of 5 and 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(model.wv.index2word) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "# l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "# l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds = model.predict_classes(X_test, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=1024)\n",
    "preds = [1 if p[1]>p[0] else 0 for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actuals = [1 if a[1]>a[0] else 0 for a in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('* 정확도 : %.5f' % (np.sum(np.array(actuals) == np.array(preds)) / float(len(actuals))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grade = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = pd.Categorical(preds, categories=grade)\n",
    "actuals = pd.Categorical(actuals, categories=grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pd.crosstab(actuals, preds, rownames=['actuals'], colnames=['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(actuals, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
